defaults:
  - _self_
  - data: multibench
  - model: mint

seed: 42
# # Add a checkpoint path here in test mode
# ckpt_path: "/path/to/best/model.pt"

mode: "train"

trainer:
  _target_: pytorch_lightning.Trainer
  accelerator: "auto" # Accelerator given to pytorch-lightning Trainer (eg `cpu` or `gpu`)
  strategy: "ddp_find_unused_parameters_true"
  devices: "auto"
  num_nodes: 1 # Number of distributed nodes
  max_epochs: 100
  default_root_dir: "."
  use_distributed_sampler: false
  deterministic: false
  inference_mode: false # avoid weird bugs during linear probing
  # Clean progress bars - Show progress but reduce verbosity
  enable_progress_bar: true  # Keep progress bars for visual feedback
  enable_model_summary: false  # Disable verbose model summary
  enable_checkpointing: true
  log_every_n_steps: 50  # Less frequent step logging
  check_val_every_n_epoch: 1  # Validate every epoch
  num_sanity_val_steps: 0  # Skip sanity validation (reduces startup noise)
  reload_dataloaders_every_n_epochs: 0  # Disable dataloader reloading messages
  # Reduce validation verbosity
  val_check_interval: 1.0  # Validate once per epoch (not multiple times)
  limit_val_batches: 1.0  # Use all validation batches but cleaner logging

linear_probing:
  _target_: evaluation.linear_probe.LinearProbingCallback
  use_sklearn: false
  fastsearch: false
  logging_level: "INFO"
  frequency: "by_epoch"

linear_probing_reg:
  _target_: evaluation.linear_probe.LinearProbingCallback
  use_sklearn: false
  fastsearch: false
  logging_level: "INFO"
  frequency: "by_epoch"

optim:
  lr: 1e-3
  weight_decay: 1e-2

mosi:
  modalities:
    - "vision"
    - "text"
  task: "classification"
  kwargs: # augmentations to apply
    augmentations: "drop+noise"
  encoders:
    - _target_: models.transformer.Transformer
      n_features: 20 # !! according to MultiBench but not in FactorCL
      dim: 40
      max_seq_length: 50
      positional_encoding: false
    - _target_: models.transformer.Transformer
      n_features: 300 # Change to fit different .pkl files. 768 for ..50.pkl, and 300 for mosi_....pkl
      dim: 40 # !! according to FactorCL paper but not in FactorCL implementation (==600)
      max_seq_length: 50
      positional_encoding: false
  adapters: # not required
    - null
    - null

mosei:
  modalities:
    - "vision"
    - "text"
  task: "classification"
  kwargs: # augmentations to apply
    augmentations: "drop+noise"
  encoders:
    - _target_: models.transformer.Transformer
      n_features: 35 # Video features from Self-MM config
      dim: 40
      max_seq_length: 50
      positional_encoding: false
    - _target_: models.transformer.Transformer
      n_features: 300 # Text features 
      dim: 40
      max_seq_length: 50
      positional_encoding: false
  adapters: # not required
    - null
    - null

humor:
  modalities:
    - "vision"
    - "text"
  task: "classification"
  kwargs: # augmentations to apply
    augmentations: "drop+drop"
  encoders:
    - _target_: models.transformer.Transformer
      n_features: 371
      dim: 40
      max_seq_length: 50
      positional_encoding: false
    - _target_: models.transformer.Transformer
      n_features: 300
      dim: 40
      max_seq_length: 50
      positional_encoding: false
  adapters: # not required
    - null
    - null

sarcasm:
  modalities:
    - "vision"
    - "text"
  task: "classification"
  kwargs: # augmentations to apply
    augmentations: "drop+drop"
  encoders:
    - _target_: models.transformer.Transformer
      n_features: 371
      dim: 40
      max_seq_length: 50
      positional_encoding: false
    - _target_: models.transformer.Transformer
      n_features: 300
      dim: 40
      max_seq_length: 50
      positional_encoding: false
  adapters: # not required
    - null
    - null