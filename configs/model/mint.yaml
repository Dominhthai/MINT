name: mint

model:
  _target_: modules.mint.MINT
  encoder: # Multi-modal model to be trained
    _target_: models.mmfusion.MMFusion
    embed_dim: 40
    fusion: "concat"
    pool: "cls"
    n_heads: 8
    n_layers: 1
    add_bias_kv: False
    dropout: 0

  projection: # Projection head 
    _target_: modules.mint.MINT._build_mlp
    in_dim: ${model.model.encoder.embed_dim} # MM encoder dimension
    mlp_dim: 512 # Hidden dim of MLP projection head
    out_dim: 256 # Output embed dim of MLP projection head

  loss_kwargs: # MINT loss
    temperature: 0.1 # Temperature in the objective function
    curriculum_weight: true # INCREASED weight for stronger teacher guidance

  pretrained_kwargs: # Self-MM style BERT+AuViSubNet teacher config
    # BERT text teacher configuration
    language: "en"
    use_finetune: true
    use_raw_text: false  # Set to true if raw text strings are available
    use_inputs_embeds: true  # Set to true if using inputs_embeds path
    # text_in allows using dataset-specific text feature size (e.g., from train_multibench.yaml encoders.n_features)
    # Set this to match cfg[dataset].encoders[TEXT].n_features
    text_in: 300
    
    # AuViSubNet vision teacher configuration (for MOSEI)
    video_in: 20     # MOSI: 20, MOSEI: 35 (using MOSEI for compatibility)
    v_lstm_hidden_size: 64  # MOSI: 64, MOSEI: 32 (using MOSEI for compatibility)
    video_out: 32    # MOSI: 32, MOSEI: 32 
    v_lstm_layers: 1
    v_lstm_dropout: 0.0
    bidirectional: false
    # Cosine similarity penalty I(Z; Y)
    alpha: 0.1
  
  curriculum_kwargs: # Curriculum learning config - Optimized for vision-weak MOSI
    v_tau: 0.0 # Moving average difficulty threshold (adaptive)
    v_lam: 0.5 # REDUCED regularization for vision (easier learning)
    v_fac: 0.95 # INCREASED momentum for vision (slower adaptation)
    t_tau: 0.0 # Moving average difficulty threshold (adaptive)
    t_lam: 1.5 # INCREASED regularization for text (harder learning)
    t_fac: 0.8 # REDUCED momentum for text (faster adaptation)